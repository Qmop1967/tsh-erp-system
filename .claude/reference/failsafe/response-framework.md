# Failsafe Response Framework

**Purpose:** Core principles and immediate response procedures for ALL system failures
**Last Updated:** 2025-11-14
**Load via:** @docs/reference/failsafe/response-framework.md

---

## ðŸŽ¯ Core Principle

**When systems fail, STABILITY and DATA INTEGRITY are priority #1.**

Never make things worse by:
- Panicking and making random changes
- Deploying "fixes" without testing
- Ignoring errors hoping they'll resolve
- Bypassing safety mechanisms
- Making irreversible data changes

---

## ðŸš¨ Immediate Response Checklist

**When ANY failure occurs, follow this exact sequence:**

### 1. STOP & ASSESS (Don't Act Immediately)

```yaml
Questions to Answer:
â–¡ What exactly failed? (specific component)
â–¡ Is this affecting users right now? (production down?)
â–¡ Is data at risk? (corruption, loss)
â–¡ What's the blast radius? (how many users affected)
â–¡ When did it start? (check logs for timestamp)
```

**PAUSE before taking action. Rushing makes things worse.**

---

### 2. ALERT USER (If Critical)

```yaml
Alert IMMEDIATELY if:
â–¡ Production system completely down
â–¡ Data corruption detected
â–¡ Security breach suspected
â–¡ Financial transaction failures
â–¡ Zoho sync completely broken (no data for hours)
â–¡ Revenue-impacting outage

Say:
"ðŸš¨ CRITICAL: [Component] is down. [Impact]. I'm investigating and will provide updates every 15 minutes."

Don't Say:
"There might be an issue..." (too vague)
"I'm not sure what's happening..." (unprofessional)
```

---

### 3. GATHER EVIDENCE (Don't Guess)

```yaml
Collect Information:
â–¡ Exact error messages (copy full text)
â–¡ Timestamps (when did it start?)
â–¡ Affected components (what's failing?)
â–¡ Recent changes (deployments, config changes)
â–¡ System status (CPU, memory, disk)
â–¡ Log files (backend, TDS Core, database)

Commands to Run:
# Check system health
curl https://erp.tsh.sale/health
curl https://tds.tsh.sale/api/health

# Check recent deployments
gh run list --limit 5

# Check logs
tail -100 /var/www/tsh-erp/logs/backend.log
tail -100 /var/www/tds-core/logs/tds_core.log

# Check database
PGPASSWORD='TSH@2025Secure!Production' psql -h localhost \
  -U tsh_app_user -d tsh_erp_production -c "SELECT COUNT(*) FROM products;"
```

---

### 4. CONTAIN DAMAGE (If Possible)

```yaml
Prevent Worse Damage:
â–¡ Stop failing process (if causing cascading failures)
â–¡ Switch to degraded mode (if available)
â–¡ Prevent data corruption (stop writes if risky)
â–¡ Isolate affected component
â–¡ Rate limit if overload

Examples:
- If API endpoint causing crashes â†’ Disable endpoint temporarily
- If background job failing â†’ Pause job scheduler
- If database writes corrupting data â†’ Enable read-only mode
- If deployment failed â†’ Rollback immediately
```

---

### 5. DIAGNOSE ROOT CAUSE

```yaml
Use Systematic Approach:
â–¡ What changed recently? (deployments, config, data)
â–¡ What do logs say? (exact error messages)
â–¡ Can you reproduce? (in staging/locally)
â–¡ What's the pattern? (intermittent vs consistent)

Apply Root-Cause Analysis:
1. State the problem clearly
2. List possible causes
3. Test each hypothesis
4. Identify true root cause
5. Don't fix symptoms, fix root cause

Reference: @docs/reference/reasoning-patterns.md
```

---

### 6. IMPLEMENT FIX (Safely)

```yaml
Safe Fix Implementation:
â–¡ Test fix in staging first (if time permits)
â–¡ Apply minimal change needed
â–¡ Have rollback plan ready
â–¡ Document what you're changing
â–¡ Monitor closely after fix
â–¡ One change at a time (don't batch fixes)

If Emergency (Production Down):
- May skip staging if critical
- But MUST document what you do
- And MUST be able to rollback
- Alert user before applying fix
```

---

### 7. VERIFY RECOVERY

```yaml
Verification Checklist:
â–¡ Core functionality works (test critical paths)
â–¡ No new errors in logs
â–¡ Performance normal (response times)
â–¡ All services running
â–¡ Zoho sync operating (if relevant)
â–¡ Users can access system

Load Full Verification:
@docs/reference/failsafe/recovery-procedures.md
```

---

### 8. DOCUMENT INCIDENT

```yaml
Record:
â–¡ What failed (symptoms, impact)
â–¡ Root cause (actual problem)
â–¡ Fix applied (exact changes)
â–¡ Verification (how you confirmed recovery)
â–¡ Prevention (how to avoid in future)
â–¡ Duration (start to resolution)

Update Knowledge Base:
@docs/reference/failsafe/failure-patterns.md
```

---

## ðŸ”´ Severity Assessment Matrix

### CRITICAL (Act Immediately)

```yaml
Indicators:
- Production completely inaccessible
- All users affected
- Data corruption in progress
- Security breach active
- Revenue-impacting failure

Response Time: < 15 minutes
Alert User: Immediately
Max Resolution Time: 2 hours
```

### HIGH (Act Within 15 Minutes)

```yaml
Indicators:
- Partial service outage
- 20%+ users affected
- Zoho sync stopped
- Major performance degradation
- Critical feature broken

Response Time: < 15 minutes
Alert User: Yes
Max Resolution Time: 4 hours
```

### MEDIUM (Act Within 1 Hour)

```yaml
Indicators:
- Single endpoint failing
- < 5% users affected
- Minor performance issues
- Non-critical service down
- Workaround available

Response Time: < 1 hour
Alert User: If noticed
Max Resolution Time: 1 business day
```

### LOW (Can Wait)

```yaml
Indicators:
- Single user issue
- Cosmetic problem
- Optional feature broken
- No user impact

Response Time: < 1 business day
Alert User: Only if asks
Max Resolution Time: 1 week
```

---

## ðŸ›¡ï¸ What You CAN Do During Failures

```yaml
âœ… SAFE OPERATIONS (Always Allowed):
- Read logs
- Query database (SELECT only)
- Check system status
- Restart services
- Rollback deployments
- Enable maintenance mode
- Disable failing components
- Scale resources (if needed)
- Contact user for guidance

âœ… CONDITIONAL (If Necessary):
- Modify configuration
- Apply emergency patches
- Database writes (if safe)
- Schema changes (if critical)

âš ï¸ ASK FIRST:
- Permanent data deletion
- Schema changes (unless critical)
- Major architectural changes
- Anything irreversible
```

---

## âŒ What You CANNOT Do During Failures

```yaml
âŒ NEVER DO:
- Panic and make random changes
- Deploy untested code to production
- Bypass security mechanisms
- Delete data without backup
- Make multiple changes at once
- Ignore errors hoping they resolve
- Skip verification steps
- Forget to alert user
- Blame external services without proof
- Give up and suggest "reinstall everything"
```

---

## ðŸ“‹ Emergency Command Reference

### System Health Checks

```bash
# Production API health
curl https://erp.tsh.sale/health

# Staging API health
curl https://staging.erp.tsh.sale/health

# TDS Core health
curl https://tds.tsh.sale/api/health

# Database check
PGPASSWORD='TSH@2025Secure!Production' psql -h localhost \
  -U tsh_app_user -d tsh_erp_production \
  -c "SELECT COUNT(*) FROM products WHERE is_active = true;"
```

### Service Management

```bash
# SSH to production
ssh root@167.71.39.50

# Check service status
systemctl status tsh-erp
systemctl status tds-core
systemctl status postgresql
systemctl status nginx

# Restart services
systemctl restart tsh-erp
systemctl restart tds-core

# View logs
journalctl -u tsh-erp -n 100 -f
journalctl -u tds-core -n 100 -f
```

### Deployment Checks

```bash
# Recent GitHub Actions
gh run list --limit 5

# Watch current deployment
gh run watch <run-id>

# View failed workflow
gh run view <run-id> --log-failed
```

---

## ðŸ”„ Escalation Path

```
You (Claude Code)
    â†“ (If can't resolve in 30 minutes)
Alert User
    â†“ (If needs server access or critical decisions)
User Takes Over
    â†“ (If beyond user's expertise)
External Expert (if needed)
```

**Key Point:** Don't waste time trying impossible fixes. Escalate when needed.

---

## âœ… Response Success Indicators

**Good Response:**
```yaml
âœ… Incident contained quickly (< 30 min)
âœ… Root cause identified correctly
âœ… Fix tested before applying
âœ… No additional damage caused
âœ… Full recovery verified
âœ… User kept informed
âœ… Documented for future
```

**Poor Response:**
```yaml
âŒ Panic and random changes
âŒ Made situation worse
âŒ Deployed untested fixes
âŒ Caused data loss
âŒ Forgot to verify
âŒ User found out from customers
âŒ Repeated same mistake
```

---

## ðŸŽ“ After Incident Resolution

```yaml
Post-Incident Actions:
1. Verify complete recovery
2. Document incident thoroughly
3. Update failure patterns knowledge base
4. Identify prevention measures
5. Implement monitoring/alerts (if missing)
6. Update relevant documentation
7. Share lessons learned

Don't Forget:
- What went well
- What went wrong
- What to improve
- How to prevent
```

---

**Related Documentation:**
- Critical scenarios: @docs/reference/failsafe/critical-scenarios/
- Recovery procedures: @docs/reference/failsafe/recovery-procedures.md
- Failure patterns: @docs/reference/failsafe/failure-patterns.md
- Emergency contacts: @docs/reference/failsafe/emergency-contacts.md
